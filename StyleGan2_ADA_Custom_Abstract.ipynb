{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StyleGan2-ADA_Custom_Abstract.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRiloKiRwOuI"
      },
      "outputs": [],
      "source": [
        "# https://github.com/dvschultz/ml-art-colabs/blob/master/Stylegan2_ada_Custom_Training.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b> Custom Training StyleGan2-ADA </b>\n",
        "StyleGAN2-ADA only work with Tensorflow 1. Run the next cell before anything else to make sure we’re using TF1 and not TF2.\n",
        "\n",
        "You're going to need at least 12 GB of RAM for the stylegan-2 to run. With Google Collab, you'd need pro or pro plus unfortunatley."
      ],
      "metadata": {
        "id": "bopj6CtXwZvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.19.5 # For some reason, need to downgrade to run."
      ],
      "metadata": {
        "id": "M_NrKnjoojez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 1.x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkBMatYUwYU_",
        "outputId": "dc0c6a53-d92f-413b-dd85-7dfa9e35cb31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkzMDN_qgFjf",
        "outputId": "5780c1c4-27db-4ad6-dcbd-3a4e8cca95f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: tensorflow\n",
            "Version: 1.15.2\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /tensorflow-1.15.2/python3.7\n",
            "Requires: google-pasta, termcolor, keras-applications, six, tensorflow-estimator, opt-einsum, protobuf, absl-py, grpcio, wheel, wrapt, astor, tensorboard, gast, keras-preprocessing, numpy\n",
            "Required-by: stable-baselines, magenta, kapre\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nzRy8sjwYaE",
        "outputId": "ce32759b-f86c-4483-e03e-ab8c7e954ec7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Mar 13 04:51:46 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   59C    P8    31W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b> Install Repo to Google Drive </b>\n",
        "Colab is a little funky with training. I’ve found the best way to do this is to install the repo directly into your Google Drive folder.\n",
        "\n",
        "First, mount your Drive to the Colab notebook:"
      ],
      "metadata": {
        "id": "kuypHwwuwmsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "fqNWuFA-wYfm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07dd1910-fb86-4c54-b5b8-bd310284b9e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, run this cell. If you’re already installed the repo, it will skip the installation process and change into the repo’s directory. If you haven’t installed it, it will install all the files necessary."
      ],
      "metadata": {
        "id": "fLbNh6sGwtPl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if os.path.isdir(\"/content/drive/My Drive/colab-sg2-ada\"):\n",
        "    %cd \"/content/drive/My Drive/colab-sg2-ada/stylegan2-ada\"\n",
        "else:\n",
        "    #install script\n",
        "    %cd \"/content/drive/My Drive/\"\n",
        "    !mkdir colab-sg2-ada\n",
        "    %cd colab-sg2-ada\n",
        "    !git clone https://github.com/dvschultz/stylegan2-ada\n",
        "    %cd stylegan2-ada\n",
        "    !mkdir downloads\n",
        "    !mkdir datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGpz2dZEwYif",
        "outputId": "7f9d7c29-5361-4ce1-be7d-e48fc15734f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/colab-sg2-ada/stylegan2-ada\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/drive/My Drive/colab-sg2-ada/stylegan2-ada\"\n",
        "!git config --global user.name \"test\"\n",
        "!git config --global user.email \"test@test.com\"\n",
        "!git fetch origin\n",
        "!git checkout origin/main -- train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CR2GzHYzwYla",
        "outputId": "ecb8b458-c877-47d6-89e2-b816e6b72036"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/colab-sg2-ada/stylegan2-ada\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b> Convert dataset to .tfrecords </b>\n",
        "<i> Note: You only need to do this once per dataset. If you have already run this and are returning to conntinue training, skip these cells. </i>\n",
        "\n",
        "Next we need to convert our image dataset to a format that StyleGAN2-ADA can read from. There are two options here. You can upload your dataset directly to Colab (as a zipped file), or you can upload it to Drive directly and read it from there."
      ],
      "metadata": {
        "id": "GCn4ktskwwhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #if you manually uploaded your dataset to Colab, unzip it\n",
        "# zip_path = \"/content/CAT1024.zip\"\n",
        "\n",
        "# !unzip {zip_path} -d /content/"
      ],
      "metadata": {
        "id": "vTYlTsfPwYon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that your image dataset is uploaded, we need to convert it to the .tfrecords format.\n",
        "\n",
        "Depending on the resolution of your images and how many you have, this can take a while."
      ],
      "metadata": {
        "id": "tlvI9qJXw3Yd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#update this to the path to your image folder\n",
        "dataset_path = \"/content/drive/MyDrive/abstract\" #/content/drive/MyDrive/abstract abstract_tf for entire ~7500 images.\n",
        "\n",
        "#give your dataset a name\n",
        "dataset_name = \"abstract_tf\" #abstract_tf for entire ~7500 images.\n",
        "\n",
        "#you don't need to edit anything here\n",
        "!python dataset_tool.py create_from_images ./datasets/{dataset_name} {dataset_path}\n",
        "\n",
        "\"\"\"NOTE: I move the ./datasets/dataset_name/ to drive/My Drive so I don't have to rerun this cell.\""
      ],
      "metadata": {
        "id": "YxOxOF0Ow2io",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bbcd3fe-a6b0-4109-bf6b-da3562836962"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading images from \"/content/drive/MyDrive/abstract_1000\"\n",
            "Creating dataset \"./datasets/abstract_tf_1000\"\n",
            "dataset_tool.py:97: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  'data': tf.train.Feature(bytes_list=tf.train.BytesList(value=[quant.tostring()]))}))\n",
            "Added 100 images.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> Train a custom model </b>\n",
        "We’re ready to start training! There are numerous arguments to training, what’s listed below are the most popular options. To see all the options, run the following cell."
      ],
      "metadata": {
        "id": "tCKzr3Q7w5mj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --help"
      ],
      "metadata": {
        "id": "7FIddY_8w2k9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e60007cf-bd01-4cf3-d9e2-db55b62e6e9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: train.py [-h] --outdir DIR [--gpus INT] [--snap INT] [--seed INT] [-n]\n",
            "                --data PATH [--res INT] [--mirror BOOL] [--mirrory BOOL]\n",
            "                [--use-raw BOOL] [--metrics LIST] [--metricdata PATH]\n",
            "                [--cfg {auto,11gb-gpu,11gb-gpu-complex,24gb-gpu,24gb-gpu-complex,48gb-gpu,48gb-2gpu,stylegan2,paper256,paper512,paper1024,cifar,cifarbaseline,aydao}]\n",
            "                [--lrate FLOAT] [--ttur BOOL] [--gamma FLOAT] [--nkimg INT]\n",
            "                [--kimg INT] [--topk FLOAT] [--aug {noaug,ada,fixed,adarv}]\n",
            "                [--p FLOAT] [--target TARGET] [--initstrength INITSTRENGTH]\n",
            "                [--augpipe {blit,geom,color,filter,noise,cutout,bg,bgc,bgcf,bgcfn,bgcfnc}]\n",
            "                [--cmethod {nocmethod,bcr,zcr,pagan,wgangp,auxrot,spectralnorm,shallowmap,adropout}]\n",
            "                [--dcap FLOAT] [--resume RESUME] [--freezed INT]\n",
            "\n",
            "Train a GAN using the techniques described in the paper\n",
            "\"Training Generative Adversarial Networks with Limited Data\".\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "\n",
            "general options:\n",
            "  --outdir DIR          Where to save the results (required)\n",
            "  --gpus INT            Number of GPUs to use (default: 1 gpu)\n",
            "  --snap INT            Snapshot interval (default: 50 ticks)\n",
            "  --seed INT            Random seed (default: 1000)\n",
            "  -n, --dry-run         Print training options and exit\n",
            "\n",
            "training dataset:\n",
            "  --data PATH           Training dataset path (required)\n",
            "  --res INT             Dataset resolution (default: highest available)\n",
            "  --mirror BOOL         Augment dataset with x-flips (default: false)\n",
            "  --mirrory BOOL        Augment dataset with y-flips (default: false)\n",
            "  --use-raw BOOL        Use raw image dataset, i.e. created from\n",
            "                        create_from_images_raw (default: False)\n",
            "\n",
            "metrics:\n",
            "  --metrics LIST        Comma-separated list or \"none\" (default: fid50k_full)\n",
            "  --metricdata PATH     Dataset to evaluate metrics against (optional)\n",
            "\n",
            "base config:\n",
            "  --cfg {auto,11gb-gpu,11gb-gpu-complex,24gb-gpu,24gb-gpu-complex,48gb-gpu,48gb-2gpu,stylegan2,paper256,paper512,paper1024,cifar,cifarbaseline,aydao}\n",
            "                        Base config (default: auto)\n",
            "  --lrate FLOAT         Override learning rate\n",
            "  --ttur BOOL           Use Two Time-Scale Update Rule (double learning rate\n",
            "                        for discriminator) (default: false)\n",
            "  --gamma FLOAT         Override R1 gamma\n",
            "  --nkimg INT           Override starting count\n",
            "  --kimg INT            Override training duration\n",
            "  --topk FLOAT          utilize top-k training\n",
            "\n",
            "discriminator augmentation:\n",
            "  --aug {noaug,ada,fixed,adarv}\n",
            "                        Augmentation mode (default: ada)\n",
            "  --p FLOAT             Specify augmentation probability for --aug=fixed\n",
            "  --target TARGET       Override ADA target for --aug=ada and --aug=adarv\n",
            "  --initstrength INITSTRENGTH\n",
            "                        Override ADA strength at start\n",
            "  --augpipe {blit,geom,color,filter,noise,cutout,bg,bgc,bgcf,bgcfn,bgcfnc}\n",
            "                        Augmentation pipeline (default: bgc)\n",
            "\n",
            "comparison methods:\n",
            "  --cmethod {nocmethod,bcr,zcr,pagan,wgangp,auxrot,spectralnorm,shallowmap,adropout}\n",
            "                        Comparison method (default: nocmethod)\n",
            "  --dcap FLOAT          Multiplier for discriminator capacity\n",
            "\n",
            "transfer learning:\n",
            "  --resume RESUME       Resume from network pickle (default: noresume)\n",
            "  --freezed INT         Freeze-D (default: 0 discriminator layers)\n",
            "\n",
            "examples:\n",
            "\n",
            "  # Train custom dataset using 1 GPU.\n",
            "  python train.py --outdir=~/training-runs --gpus=1 --data=~/datasets/custom\n",
            "\n",
            "  # Train class-conditional CIFAR-10 using 2 GPUs.\n",
            "  python train.py --outdir=~/training-runs --gpus=2 --data=~/datasets/cifar10c \\\n",
            "      --cfg=cifar\n",
            "\n",
            "  # Transfer learn MetFaces from FFHQ using 4 GPUs.\n",
            "  python train.py --outdir=~/training-runs --gpus=4 --data=~/datasets/metfaces \\\n",
            "      --cfg=paper1024 --mirror=1 --resume=ffhq1024 --snap=10\n",
            "\n",
            "  # Reproduce original StyleGAN2 config F.\n",
            "  python train.py --outdir=~/training-runs --gpus=8 --data=~/datasets/ffhq \\\n",
            "      --cfg=stylegan2 --res=1024 --mirror=1 --aug=noaug\n",
            "\n",
            "available base configs (--cfg):\n",
            "  auto           Automatically select reasonable defaults based on resolution\n",
            "                 and GPU count. Good starting point for new datasets.\n",
            "  stylegan2      Reproduce results for StyleGAN2 config F at 1024x1024.\n",
            "  paper256       Reproduce results for FFHQ and LSUN Cat at 256x256.\n",
            "  paper512       Reproduce results for BreCaHAD and AFHQ at 512x512.\n",
            "  paper1024      Reproduce results for MetFaces at 1024x1024.\n",
            "  cifar          Reproduce results for CIFAR-10 (tuned configuration).\n",
            "  cifarbaseline  Reproduce results for CIFAR-10 (baseline configuration).\n",
            "\n",
            "transfer learning source networks (--resume):\n",
            "  ffhq256        FFHQ trained at 256x256 resolution.\n",
            "  ffhq512        FFHQ trained at 512x512 resolution.\n",
            "  ffhq1024       FFHQ trained at 1024x1024 resolution.\n",
            "  celebahq256    CelebA-HQ trained at 256x256 resolution.\n",
            "  lsundog256     LSUN Dog trained at 256x256 resolution.\n",
            "  afhqcat512     AFHQ Cat trained at 512x512 resolution.\n",
            "  afhqdog512     AFHQ Dog trained at 512x512 resolution.\n",
            "  afhqwild512    AFHQ Wild trained at 512x512 resolution.\n",
            "  brecahad512    BreCaHAD trained at 512x512 resolution.\n",
            "  cifar10        CIFAR10 trained at 32x32 resolution.\n",
            "  metfaces512    MetFaces trained at 512x512 resolution.\n",
            "  <path or URL>  Custom network pickle.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#this name must EXACTLY match the dataset name you used when creating the .tfrecords file\n",
        "dataset_name = \"abstract_tf_1000\" # abstract_tf\n",
        "#how often should the model generate samples and a .pkl file\n",
        "snapshot_count = 5\n",
        "#should the images be mirrored left to right?\n",
        "mirrored = True\n",
        "#should the images be mirrored top to bottom?\n",
        "mirroredY = True\n",
        "\"\"\"Since mirrored vertically and horizontally, we now have x4 as many records.\"\"\"\n",
        "\n",
        "#metrics? \n",
        "metric_list = None\n",
        "#augments\n",
        "augs = \"bgcfnc\"\n",
        "\n",
        "#\n",
        "# this is the most important cell to update\n",
        "#\n",
        "# running it for the first time? set it to ffhq(+resolution)\n",
        "# resuming? get the path to your latest .pkl file and use that\n",
        "# resume_from = \"/content/drive/My\\ Drive/colab-sg2-ada2/stylegan2-ada/results/00008-dante1024-mirror-mirrory-11gb-gpu-bg-resumecustom/network-snapshot-000160.pkl\"\n",
        "# resume_from = 'ffhq256' --resume --resume={resume_from} \n",
        "#!python train.py --outdir /content/drive/MyDrive/results_abstract_gan --snap={snapshot_count} --cfg=auto --data=/content/drive/MyDrive/{dataset_name} --augpipe={augs} --mirror={mirrored} --mirrory={mirroredY} --metrics={metric_list}\n",
        "!python train.py --outdir=/content/drive/MyDrive/results_abstract_gan --snap={snapshot_count} --gpus=1 --data=/content/drive/MyDrive/{dataset_name} --kimg=3000 --aug=ada"
      ],
      "metadata": {
        "id": "aMVczuUtw2nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3e80e14-f5f5-4087-95e0-28a71fe289ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tcmalloc: large alloc 4294967296 bytes == 0x55814b8d8000 @  0x7f4804900001 0x7f4801b0354f 0x7f4801b53b58 0x7f4801b57b17 0x7f4801bf6203 0x558143b6f424 0x558143b6f120 0x558143be3b80 0x558143bde66e 0x558143b7136c 0x558143bb27b9 0x558143baf6d4 0x558143b6fc29 0x558143be3e61 0x558143bde02f 0x558143aafe2b 0x558143be0633 0x558143bde02f 0x558143aafe2b 0x558143be0633 0x558143bde66e 0x558143aafe2b 0x558143be0633 0x558143b709da 0x558143bdeeae 0x558143bde02f 0x558143bddd43 0x558143ca8302 0x558143ca867d 0x558143ca8526 0x558143c801d3\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x55824b8d8000 @  0x7f48048fe1e7 0x7f4801b0346e 0x7f4801b53c7b 0x7f4801b5435f 0x7f4801bf6103 0x558143b6f424 0x558143b6f120 0x558143be3b80 0x558143bde02f 0x558143b70aba 0x558143bdfcd4 0x558143bde02f 0x558143b70aba 0x558143bdfcd4 0x558143bde02f 0x558143b70aba 0x558143bdfcd4 0x558143b709da 0x558143bdeeae 0x558143bde02f 0x558143b70aba 0x558143be32c0 0x558143bde02f 0x558143b70aba 0x558143bdfcd4 0x558143bde66e 0x558143b7136c 0x558143bb27b9 0x558143baf6d4 0x558143b6fc29 0x558143be3e61\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x55834c1b4000 @  0x7f48048fe1e7 0x7f4801b0346e 0x7f4801b53c7b 0x7f4801b5435f 0x7f47be505235 0x7f47bde88792 0x7f47bde88d42 0x7f47bde41aee 0x558143b6f317 0x558143b6f120 0x558143be3679 0x558143b709da 0x558143bdf108 0x558143bde1c0 0x558143aafeb0 0x558143be0633 0x558143bde02f 0x558143b70aba 0x558143bdf108 0x558143bde66e 0x558143b70aba 0x558143bdf108 0x558143b709da 0x558143bdf108 0x558143bde02f 0x558143b71151 0x558143b71571 0x558143be0633 0x558143bde02f 0x558143b70aba 0x558143bdeeae\n",
            "\n",
            "Training options:\n",
            "{\n",
            "  \"G_args\": {\n",
            "    \"func_name\": \"training.networks.G_main\",\n",
            "    \"fmap_base\": 8192,\n",
            "    \"fmap_max\": 512,\n",
            "    \"mapping_layers\": 2,\n",
            "    \"num_fp16_res\": 4,\n",
            "    \"conv_clamp\": 256\n",
            "  },\n",
            "  \"D_args\": {\n",
            "    \"func_name\": \"training.networks.D_main\",\n",
            "    \"mbstd_group_size\": 4,\n",
            "    \"fmap_base\": 8192,\n",
            "    \"fmap_max\": 512,\n",
            "    \"num_fp16_res\": 4,\n",
            "    \"conv_clamp\": 256\n",
            "  },\n",
            "  \"G_opt_args\": {\n",
            "    \"beta1\": 0.0,\n",
            "    \"beta2\": 0.99,\n",
            "    \"learning_rate\": 0.0025\n",
            "  },\n",
            "  \"D_opt_args\": {\n",
            "    \"beta1\": 0.0,\n",
            "    \"beta2\": 0.99,\n",
            "    \"learning_rate\": 0.0025\n",
            "  },\n",
            "  \"loss_args\": {\n",
            "    \"func_name\": \"training.loss.stylegan2\",\n",
            "    \"r1_gamma\": 0.8192\n",
            "  },\n",
            "  \"augment_args\": {\n",
            "    \"class_name\": \"training.augment.AdaptiveAugment\",\n",
            "    \"tune_heuristic\": \"rt\",\n",
            "    \"tune_target\": 0.6,\n",
            "    \"apply_func\": \"training.augment.augment_pipeline\",\n",
            "    \"apply_args\": {\n",
            "      \"xflip\": 1,\n",
            "      \"rotate90\": 1,\n",
            "      \"xint\": 1,\n",
            "      \"scale\": 1,\n",
            "      \"rotate\": 1,\n",
            "      \"aniso\": 1,\n",
            "      \"xfrac\": 1,\n",
            "      \"brightness\": 1,\n",
            "      \"contrast\": 1,\n",
            "      \"lumaflip\": 1,\n",
            "      \"hue\": 1,\n",
            "      \"saturation\": 1\n",
            "    }\n",
            "  },\n",
            "  \"num_gpus\": 1,\n",
            "  \"image_snapshot_ticks\": 5,\n",
            "  \"network_snapshot_ticks\": 5,\n",
            "  \"train_dataset_args\": {\n",
            "    \"path\": \"/content/drive/MyDrive/abstract_tf_1000\",\n",
            "    \"max_label_size\": 0,\n",
            "    \"use_raw\": false,\n",
            "    \"resolution\": 256,\n",
            "    \"mirror_augment\": false,\n",
            "    \"mirror_augment_v\": false\n",
            "  },\n",
            "  \"metric_arg_list\": [\n",
            "    {\n",
            "      \"name\": \"fid50k_full\",\n",
            "      \"class_name\": \"metrics.frechet_inception_distance.FID\",\n",
            "      \"max_reals\": null,\n",
            "      \"num_fakes\": 50000,\n",
            "      \"minibatch_per_gpu\": 8,\n",
            "      \"force_dataset_args\": {\n",
            "        \"shuffle\": false,\n",
            "        \"max_images\": null,\n",
            "        \"repeat\": false,\n",
            "        \"mirror_augment\": false\n",
            "      }\n",
            "    }\n",
            "  ],\n",
            "  \"metric_dataset_args\": {\n",
            "    \"path\": \"/content/drive/MyDrive/abstract_tf_1000\",\n",
            "    \"max_label_size\": 0,\n",
            "    \"use_raw\": false,\n",
            "    \"resolution\": 256,\n",
            "    \"mirror_augment\": false,\n",
            "    \"mirror_augment_v\": false\n",
            "  },\n",
            "  \"total_kimg\": 3000,\n",
            "  \"minibatch_size\": 16,\n",
            "  \"minibatch_gpu\": 16,\n",
            "  \"G_smoothing_kimg\": 5.0,\n",
            "  \"G_smoothing_rampup\": 0.05,\n",
            "  \"run_dir\": \"/content/drive/MyDrive/results_abstract_gan/00009-abstract_tf_1000-auto1-kimg3000-ada\"\n",
            "}\n",
            "\n",
            "Output directory:  /content/drive/MyDrive/results_abstract_gan/00009-abstract_tf_1000-auto1-kimg3000-ada\n",
            "Training data:     /content/drive/MyDrive/abstract_tf_1000\n",
            "Training length:   3000 kimg\n",
            "Resolution:        256\n",
            "Number of GPUs:    1\n",
            "\n",
            "Creating output directory...\n",
            "Loading training set...\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x55844c4a8000 @  0x7f4804900001 0x7f4801b0354f 0x7f4801b53b58 0x7f4801b57b17 0x7f4801bf6203 0x558143b6f424 0x558143b6f120 0x558143be3b80 0x558143bde66e 0x558143b7136c 0x558143bb27b9 0x558143baf6d4 0x558143b6fc29 0x558143be3e61 0x558143bde02f 0x558143aafe2b 0x558143be0633 0x558143bde02f 0x558143aafe2b 0x558143be0633 0x558143bde66e 0x558143aafe2b 0x558143be0633 0x558143b709da 0x558143bdeeae 0x558143bde02f 0x558143bddd43 0x558143ca8302 0x558143ca867d 0x558143ca8526 0x558143c801d3\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x55854ccb4000 @  0x7f48048fe1e7 0x7f4801b0346e 0x7f4801b53c7b 0x7f4801b5435f 0x7f4801bf6103 0x558143b6f424 0x558143b6f120 0x558143be3b80 0x558143bde02f 0x558143b70aba 0x558143bdfcd4 0x558143bde02f 0x558143b70aba 0x558143bdfcd4 0x558143bde02f 0x558143b70aba 0x558143bdfcd4 0x558143b709da 0x558143bdeeae 0x558143bde02f 0x558143b70aba 0x558143be32c0 0x558143bde02f 0x558143b70aba 0x558143bdfcd4 0x558143bde66e 0x558143b7136c 0x558143bb27b9 0x558143baf6d4 0x558143b6fc29 0x558143be3e61\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x55854ccb4000 @  0x7f48048fe1e7 0x7f4801b0346e 0x7f4801b53c7b 0x7f4801b5435f 0x7f47be505235 0x7f47bde88792 0x7f47bde88d42 0x7f47bde41aee 0x558143b6f317 0x558143b6f120 0x558143be3679 0x558143b709da 0x558143bdf108 0x558143bde1c0 0x558143aafeb0 0x558143be0633 0x558143bde02f 0x558143b70aba 0x558143bdf108 0x558143bde66e 0x558143b70aba 0x558143bdf108 0x558143b709da 0x558143bdf108 0x558143bde02f 0x558143b71151 0x558143b71571 0x558143be0633 0x558143bde02f 0x558143b70aba 0x558143bdeeae\n",
            "Image shape: [3, 256, 256]\n",
            "Label shape: [0]\n",
            "\n",
            "Constructing networks...\n",
            "Setting up TensorFlow plugin \"fused_bias_act.cu\": Loading... Done.\n",
            "Setting up TensorFlow plugin \"upfirdn_2d.cu\": Loading... Done.\n",
            "\n",
            "G                             Params    OutputShape         WeightShape     \n",
            "---                           ---       ---                 ---             \n",
            "latents_in                    -         (?, 512)            -               \n",
            "labels_in                     -         (?, 0)              -               \n",
            "epochs                        1         ()                  ()              \n",
            "epochs_1                      1         ()                  ()              \n",
            "G_mapping/Normalize           -         (?, 512)            -               \n",
            "G_mapping/Dense0              262656    (?, 512)            (512, 512)      \n",
            "G_mapping/Dense1              262656    (?, 512)            (512, 512)      \n",
            "G_mapping/Broadcast           -         (?, 14, 512)        -               \n",
            "dlatent_avg                   -         (512,)              -               \n",
            "Truncation/Lerp               -         (?, 14, 512)        -               \n",
            "G_synthesis/4x4/Const         8192      (?, 512, 4, 4)      (1, 512, 4, 4)  \n",
            "G_synthesis/4x4/Conv          2622465   (?, 512, 4, 4)      (3, 3, 512, 512)\n",
            "G_synthesis/4x4/ToRGB         264195    (?, 3, 4, 4)        (1, 1, 512, 3)  \n",
            "G_synthesis/8x8/Conv0_up      2622465   (?, 512, 8, 8)      (3, 3, 512, 512)\n",
            "G_synthesis/8x8/Conv1         2622465   (?, 512, 8, 8)      (3, 3, 512, 512)\n",
            "G_synthesis/8x8/Upsample      -         (?, 3, 8, 8)        -               \n",
            "G_synthesis/8x8/ToRGB         264195    (?, 3, 8, 8)        (1, 1, 512, 3)  \n",
            "G_synthesis/16x16/Conv0_up    2622465   (?, 512, 16, 16)    (3, 3, 512, 512)\n",
            "G_synthesis/16x16/Conv1       2622465   (?, 512, 16, 16)    (3, 3, 512, 512)\n",
            "G_synthesis/16x16/Upsample    -         (?, 3, 16, 16)      -               \n",
            "G_synthesis/16x16/ToRGB       264195    (?, 3, 16, 16)      (1, 1, 512, 3)  \n",
            "G_synthesis/32x32/Conv0_up    2622465   (?, 512, 32, 32)    (3, 3, 512, 512)\n",
            "G_synthesis/32x32/Conv1       2622465   (?, 512, 32, 32)    (3, 3, 512, 512)\n",
            "G_synthesis/32x32/Upsample    -         (?, 3, 32, 32)      -               \n",
            "G_synthesis/32x32/ToRGB       264195    (?, 3, 32, 32)      (1, 1, 512, 3)  \n",
            "G_synthesis/64x64/Conv0_up    1442561   (?, 256, 64, 64)    (3, 3, 512, 256)\n",
            "G_synthesis/64x64/Conv1       721409    (?, 256, 64, 64)    (3, 3, 256, 256)\n",
            "G_synthesis/64x64/Upsample    -         (?, 3, 64, 64)      -               \n",
            "G_synthesis/64x64/ToRGB       132099    (?, 3, 64, 64)      (1, 1, 256, 3)  \n",
            "G_synthesis/128x128/Conv0_up  426369    (?, 128, 128, 128)  (3, 3, 256, 128)\n",
            "G_synthesis/128x128/Conv1     213249    (?, 128, 128, 128)  (3, 3, 128, 128)\n",
            "G_synthesis/128x128/Upsample  -         (?, 3, 128, 128)    -               \n",
            "G_synthesis/128x128/ToRGB     66051     (?, 3, 128, 128)    (1, 1, 128, 3)  \n",
            "G_synthesis/256x256/Conv0_up  139457    (?, 64, 256, 256)   (3, 3, 128, 64) \n",
            "G_synthesis/256x256/Conv1     69761     (?, 64, 256, 256)   (3, 3, 64, 64)  \n",
            "G_synthesis/256x256/Upsample  -         (?, 3, 256, 256)    -               \n",
            "G_synthesis/256x256/ToRGB     33027     (?, 3, 256, 256)    (1, 1, 64, 3)   \n",
            "---                           ---       ---                 ---             \n",
            "Total                         23191524                                      \n",
            "\n",
            "\n",
            "D                    Params    OutputShape         WeightShape     \n",
            "---                  ---       ---                 ---             \n",
            "images_in            -         (?, 3, 256, 256)    -               \n",
            "labels_in            -         (?, 0)              -               \n",
            "256x256/FromRGB      256       (?, 64, 256, 256)   (1, 1, 3, 64)   \n",
            "256x256/Conv0        36928     (?, 64, 256, 256)   (3, 3, 64, 64)  \n",
            "256x256/Conv1_down   73856     (?, 128, 128, 128)  (3, 3, 64, 128) \n",
            "256x256/Skip         8192      (?, 128, 128, 128)  (1, 1, 64, 128) \n",
            "128x128/Conv0        147584    (?, 128, 128, 128)  (3, 3, 128, 128)\n",
            "128x128/Conv1_down   295168    (?, 256, 64, 64)    (3, 3, 128, 256)\n",
            "128x128/Skip         32768     (?, 256, 64, 64)    (1, 1, 128, 256)\n",
            "64x64/Conv0          590080    (?, 256, 64, 64)    (3, 3, 256, 256)\n",
            "64x64/Conv1_down     1180160   (?, 512, 32, 32)    (3, 3, 256, 512)\n",
            "64x64/Skip           131072    (?, 512, 32, 32)    (1, 1, 256, 512)\n",
            "32x32/Conv0          2359808   (?, 512, 32, 32)    (3, 3, 512, 512)\n",
            "32x32/Conv1_down     2359808   (?, 512, 16, 16)    (3, 3, 512, 512)\n",
            "32x32/Skip           262144    (?, 512, 16, 16)    (1, 1, 512, 512)\n",
            "16x16/Conv0          2359808   (?, 512, 16, 16)    (3, 3, 512, 512)\n",
            "16x16/Conv1_down     2359808   (?, 512, 8, 8)      (3, 3, 512, 512)\n",
            "16x16/Skip           262144    (?, 512, 8, 8)      (1, 1, 512, 512)\n",
            "8x8/Conv0            2359808   (?, 512, 8, 8)      (3, 3, 512, 512)\n",
            "8x8/Conv1_down       2359808   (?, 512, 4, 4)      (3, 3, 512, 512)\n",
            "8x8/Skip             262144    (?, 512, 4, 4)      (1, 1, 512, 512)\n",
            "4x4/MinibatchStddev  -         (?, 513, 4, 4)      -               \n",
            "4x4/Conv             2364416   (?, 512, 4, 4)      (3, 3, 513, 512)\n",
            "4x4/Dense0           4194816   (?, 512)            (8192, 512)     \n",
            "Output               513       (?, 1)              (512, 1)        \n",
            "---                  ---       ---                 ---             \n",
            "Total                24001089                                      \n",
            "\n",
            "Exporting sample images...\n",
            "Replicating networks across 1 GPUs...\n",
            "Initializing augmentations...\n",
            "Setting up optimizers...\n",
            "Constructing training graph...\n",
            "Finalizing training ops...\n",
            "Initializing metrics...\n",
            "Training for 3000 kimg...\n",
            "\n",
            "tick 0     kimg 0.1      time 2m 10s       sec/tick 43.2    sec/kimg 674.29  maintenance 87.3   gpumem 7.3   augment 0.000\n",
            "Evaluating metrics...\n",
            "Calculating real image statistics for fid50k_full...\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x55864d4b4000 @  0x7f4804900001 0x7f4801b0354f 0x7f4801b53b58 0x7f4801b57b17 0x7f4801bf6203 0x558143b6f424 0x558143b6f120 0x558143be3b80 0x558143bde66e 0x558143b7136c 0x558143bb27b9 0x558143baf6d4 0x558143b6fc29 0x558143be3e61 0x558143bde02f 0x558143aafe2b 0x558143be0633 0x558143b709da 0x558143bdf108 0x558143c62a18 0x558143bdf350 0x558143bde02f 0x558143b70aba 0x558143bdfcd4 0x558143bde02f 0x558143b70aba 0x558143bdfcd4 0x558143bde02f 0x558143aafe2b 0x558143be0633 0x558143bde66e\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "While it’s training...\n",
        "Once the above cell is running you should be training!\n",
        "\n",
        "Don’t close this tab! Colab needs to be open and running in order to continue training. Every ~15min or so a new line should get added to your output, indicated its still training. Depending on you snapshot_count setting you should see the results folder in your Google drive folder fill with both samples (fakesXXXXXx.jpg) and model weights (network-snapshot-XXXXXX.pkl). The samples are worth looking at while it trains but don’t get too worried about each individual sample.\n",
        "\n",
        "If you chose a metric, you will also see scores for each snapshot. Don’t obsess over these! they are a guide, it can go up or down slightly for each snapshot. What you want to see is a gradual lowering of the score over time.\n",
        "\n",
        "Once Colab shuts off, you can Reconnect the notebook and re-run every cell from top to bottom. Make sure you update the resume_from path to continue training from the latest model."
      ],
      "metadata": {
        "id": "kWRzjrLMxBGT"
      }
    }
  ]
}